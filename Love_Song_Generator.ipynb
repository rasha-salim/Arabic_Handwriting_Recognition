{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Love_Song_Generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPz3OqOre4IDbGJ9Obesdwd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gamesMum/Arabic_Handwriting_Recognition/blob/master/Love_Song_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92Ptj_ps7N_D",
        "colab_type": "text"
      },
      "source": [
        "Importing the data from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NIku7Fk6iMh",
        "colab_type": "code",
        "outputId": "a5c552e8-02cd-41c9-c59d-0e08ab1ac121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43xvpcl58awE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing some importent libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFqFcjsY7wwk",
        "colab_type": "code",
        "outputId": "abbe99b6-62d6-4a61-9c78-7d973f76e59c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Let's open the text file and read it\n",
        "with open('/content/drive/My Drive/data/love_songs.txt', 'r') as f:\n",
        "  text = f.read()\n",
        "\n",
        "text[:100]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Doesn't take much to make me happy\\nAnd make me smile with glee \\nNever never will I feel discouraged \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byR8A5bp89pR",
        "colab_type": "text"
      },
      "source": [
        "## **Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKnooBRs8zQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Encode the text and map each character to an integer and vice versa\n",
        "chars = tuple(set(text)) #search this\n",
        "int2char = dict(enumerate(chars)) #keys are integer, values are chars\n",
        "char2int = {ch: ii for ii, ch in int2char.items()} #keys are characters, values are values\n",
        "\n",
        "#encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d76ipB2097RF",
        "colab_type": "code",
        "outputId": "5456184a-11e7-47a9-e490-eb69663d6380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encoded[:100]\n",
        "print()\n",
        "encoded.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(902550,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gHmiYq1--mg",
        "colab_type": "text"
      },
      "source": [
        "# **Pre-processing the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xP0fxc1-IeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "  #initialize the encoded array with zeros\n",
        "  one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "\n",
        "  #fill the approperiate elemnts with ones\n",
        "  one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1 #flatten makes 1D array\n",
        "\n",
        "  #reshape it to the oriinal array\n",
        "  one_hot = one_hot.reshape((*arr.shape, n_labels)) #why th *\n",
        "\n",
        "  return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VtN-gg7AaYr",
        "colab_type": "code",
        "outputId": "838e9124-0267-4f15-e40b-200f5250080a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Test\n",
        "# check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkK_hJ8VAm5q",
        "colab_type": "text"
      },
      "source": [
        "## **Making the min-batching**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGQg3SZPAhJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "  #number of characters in a complete min batch\n",
        "  batch_size_total = batch_size * seq_length\n",
        "\n",
        "  #Get the number of batches\n",
        "  n_batches = len(arr) // batch_size_total\n",
        "\n",
        "  #keep only enough characters to make full batches\n",
        "  arr = arr[: n_batches * batch_size_total]\n",
        "  #reshape into batch_size row\n",
        "  arr = arr.reshape(batch_size, -1)\n",
        "  #Iterate through the batches using window of size seq_length\n",
        "  for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features (rows are the batches, columns are the seq_length window)\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "          y[:, :-1], y[:, -1] = x[: , 1:], arr[:, n+seq_length] #y = x hifted by one\n",
        "        except IndexError:\n",
        "          y[:, :-1], y[:, -1] = x[: , 1:], arr[:, 0]\n",
        "        yield x, y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBuzL1UxMxBn",
        "colab_type": "text"
      },
      "source": [
        "## **Define the network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnxS4FPrFpaq",
        "colab_type": "code",
        "outputId": "659c0d05-cd24-49f0-b2f6-fe09695ed31c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        " #check if gpu is available\n",
        " train_on_gpu = torch.cuda.is_available()\n",
        " if(train_on_gpu):\n",
        "   print('Training on GPU')\n",
        " else:\n",
        "   print('No GPU is available. Trainig on CPU')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP8Tf0ZV1KOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define our Networ\n",
        "class CharRNN(nn.Module):\n",
        "  def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "               drop_prob=0.25, lr=0.01):\n",
        "    super().__init__()\n",
        "    self.drop_prob = drop_prob\n",
        "    self.n_layers = n_layers\n",
        "    self.n_hidden = n_hidden\n",
        "    self.lr = lr\n",
        "\n",
        "    #create charachters dictionary\n",
        "    self.chars =  tokens\n",
        "    self.int2char = dict(enumerate(self.chars))\n",
        "    self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "    #Define the layers of the model\n",
        "    self.lstm = nn.LSTM(len(chars), n_hidden, n_layers,\n",
        "                        dropout=drop_prob, batch_first=True)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "    self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    # x (batch_size, seq_length, input_size)\n",
        "    # hidden (n_layers, batch_size, hidden_dim)\n",
        "    # r_out (batch_size, time_step, hidden_size)   \n",
        "    ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "\n",
        "    #get LSTM outputs\n",
        "    r_out, hidden = self.lstm(x, hidden)\n",
        "    #pass it through the dropout layer\n",
        "    out = self.dropout(r_out)\n",
        "\n",
        "    #stack up LSTM outputs using view \n",
        "    #use contiguous to reashape the output\n",
        "    out = out.contiguous().view(-1, self.n_hidden)\n",
        "    #now pass it through the fully connected layer\n",
        "    out = self.fc(out)\n",
        "\n",
        "    #return the final output and the hidden state\n",
        "    return out, hidden\n",
        "  def init_hidden(self, batch_size):\n",
        "    # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "    # initialized to zero, for hidden state and cell state of LSTM\n",
        "    weight = next(self.parameters()).data\n",
        "\n",
        "    if (train_on_gpu):\n",
        "       hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "    else:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "    return hidden\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shefeQSTDsGf",
        "colab_type": "text"
      },
      "source": [
        "## **Train our model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLylwKWNCy2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h) #h: contains all information from all the previous steps\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvEg0OXnD0ZC",
        "colab_type": "code",
        "outputId": "0446cfdb-5db8-4db4-97db-61cca7d8354e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# define and print the net\n",
        "n_hidden= 128\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers, drop_prob=0.25)\n",
        "print(net)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(99, 128, num_layers=2, batch_first=True, dropout=0.25)\n",
            "  (dropout): Dropout(p=0.25, inplace=False)\n",
            "  (fc): Linear(in_features=128, out_features=99, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHxRDnwjD8BA",
        "colab_type": "code",
        "outputId": "d54a881d-d9cf-4634-8603-929165b265ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 100 \n",
        "seq_length = 64\n",
        "n_epochs =  50 # start small if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.002, print_every=10)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/50... Step: 10... Loss: 3.2930... Val Loss: 3.2831\n",
            "Epoch: 1/50... Step: 20... Loss: 3.1826... Val Loss: 3.2226\n",
            "Epoch: 1/50... Step: 30... Loss: 3.1601... Val Loss: 3.2035\n",
            "Epoch: 1/50... Step: 40... Loss: 3.1499... Val Loss: 3.1960\n",
            "Epoch: 1/50... Step: 50... Loss: 3.1475... Val Loss: 3.1958\n",
            "Epoch: 1/50... Step: 60... Loss: 3.1381... Val Loss: 3.1930\n",
            "Epoch: 1/50... Step: 70... Loss: 3.1485... Val Loss: 3.1934\n",
            "Epoch: 1/50... Step: 80... Loss: 3.1378... Val Loss: 3.1924\n",
            "Epoch: 1/50... Step: 90... Loss: 3.1709... Val Loss: 3.1922\n",
            "Epoch: 1/50... Step: 100... Loss: 3.1276... Val Loss: 3.1898\n",
            "Epoch: 1/50... Step: 110... Loss: 3.1533... Val Loss: 3.1868\n",
            "Epoch: 1/50... Step: 120... Loss: 3.1318... Val Loss: 3.1787\n",
            "Epoch: 2/50... Step: 130... Loss: 3.1249... Val Loss: 3.1647\n",
            "Epoch: 2/50... Step: 140... Loss: 3.0818... Val Loss: 3.1340\n",
            "Epoch: 2/50... Step: 150... Loss: 3.0097... Val Loss: 3.0830\n",
            "Epoch: 2/50... Step: 160... Loss: 2.9631... Val Loss: 3.0176\n",
            "Epoch: 2/50... Step: 170... Loss: 2.9212... Val Loss: 2.9597\n",
            "Epoch: 2/50... Step: 180... Loss: 2.8417... Val Loss: 2.9121\n",
            "Epoch: 2/50... Step: 190... Loss: 2.8109... Val Loss: 2.8649\n",
            "Epoch: 2/50... Step: 200... Loss: 2.7739... Val Loss: 2.8191\n",
            "Epoch: 2/50... Step: 210... Loss: 2.7348... Val Loss: 2.7690\n",
            "Epoch: 2/50... Step: 220... Loss: 2.6606... Val Loss: 2.7301\n",
            "Epoch: 2/50... Step: 230... Loss: 2.6214... Val Loss: 2.6897\n",
            "Epoch: 2/50... Step: 240... Loss: 2.5746... Val Loss: 2.6545\n",
            "Epoch: 2/50... Step: 250... Loss: 2.5740... Val Loss: 2.6221\n",
            "Epoch: 3/50... Step: 260... Loss: 2.5233... Val Loss: 2.5947\n",
            "Epoch: 3/50... Step: 270... Loss: 2.5278... Val Loss: 2.5661\n",
            "Epoch: 3/50... Step: 280... Loss: 2.4863... Val Loss: 2.5411\n",
            "Epoch: 3/50... Step: 290... Loss: 2.4372... Val Loss: 2.5174\n",
            "Epoch: 3/50... Step: 300... Loss: 2.4273... Val Loss: 2.4935\n",
            "Epoch: 3/50... Step: 310... Loss: 2.4361... Val Loss: 2.4796\n",
            "Epoch: 3/50... Step: 320... Loss: 2.3908... Val Loss: 2.4558\n",
            "Epoch: 3/50... Step: 330... Loss: 2.3220... Val Loss: 2.4439\n",
            "Epoch: 3/50... Step: 340... Loss: 2.3229... Val Loss: 2.4215\n",
            "Epoch: 3/50... Step: 350... Loss: 2.3678... Val Loss: 2.4010\n",
            "Epoch: 3/50... Step: 360... Loss: 2.3225... Val Loss: 2.3844\n",
            "Epoch: 3/50... Step: 370... Loss: 2.3134... Val Loss: 2.3729\n",
            "Epoch: 4/50... Step: 380... Loss: 2.2933... Val Loss: 2.3601\n",
            "Epoch: 4/50... Step: 390... Loss: 2.2342... Val Loss: 2.3508\n",
            "Epoch: 4/50... Step: 400... Loss: 2.2554... Val Loss: 2.3336\n",
            "Epoch: 4/50... Step: 410... Loss: 2.2507... Val Loss: 2.3270\n",
            "Epoch: 4/50... Step: 420... Loss: 2.2173... Val Loss: 2.3118\n",
            "Epoch: 4/50... Step: 430... Loss: 2.2193... Val Loss: 2.2993\n",
            "Epoch: 4/50... Step: 440... Loss: 2.2005... Val Loss: 2.2882\n",
            "Epoch: 4/50... Step: 450... Loss: 2.2335... Val Loss: 2.2752\n",
            "Epoch: 4/50... Step: 460... Loss: 2.1360... Val Loss: 2.2687\n",
            "Epoch: 4/50... Step: 470... Loss: 2.1388... Val Loss: 2.2546\n",
            "Epoch: 4/50... Step: 480... Loss: 2.1522... Val Loss: 2.2421\n",
            "Epoch: 4/50... Step: 490... Loss: 2.1507... Val Loss: 2.2342\n",
            "Epoch: 4/50... Step: 500... Loss: 2.1606... Val Loss: 2.2246\n",
            "Epoch: 5/50... Step: 510... Loss: 2.0858... Val Loss: 2.2150\n",
            "Epoch: 5/50... Step: 520... Loss: 2.1084... Val Loss: 2.2049\n",
            "Epoch: 5/50... Step: 530... Loss: 2.0755... Val Loss: 2.1999\n",
            "Epoch: 5/50... Step: 540... Loss: 2.0745... Val Loss: 2.1881\n",
            "Epoch: 5/50... Step: 550... Loss: 2.0940... Val Loss: 2.1794\n",
            "Epoch: 5/50... Step: 560... Loss: 2.0143... Val Loss: 2.1703\n",
            "Epoch: 5/50... Step: 570... Loss: 2.0700... Val Loss: 2.1628\n",
            "Epoch: 5/50... Step: 580... Loss: 2.0179... Val Loss: 2.1542\n",
            "Epoch: 5/50... Step: 590... Loss: 2.0418... Val Loss: 2.1462\n",
            "Epoch: 5/50... Step: 600... Loss: 2.0448... Val Loss: 2.1351\n",
            "Epoch: 5/50... Step: 610... Loss: 2.0364... Val Loss: 2.1291\n",
            "Epoch: 5/50... Step: 620... Loss: 1.9913... Val Loss: 2.1222\n",
            "Epoch: 5/50... Step: 630... Loss: 2.0398... Val Loss: 2.1204\n",
            "Epoch: 6/50... Step: 640... Loss: 2.0364... Val Loss: 2.1104\n",
            "Epoch: 6/50... Step: 650... Loss: 2.0127... Val Loss: 2.1044\n",
            "Epoch: 6/50... Step: 660... Loss: 1.9911... Val Loss: 2.1017\n",
            "Epoch: 6/50... Step: 670... Loss: 1.9903... Val Loss: 2.0933\n",
            "Epoch: 6/50... Step: 680... Loss: 1.9380... Val Loss: 2.0854\n",
            "Epoch: 6/50... Step: 690... Loss: 1.9504... Val Loss: 2.0812\n",
            "Epoch: 6/50... Step: 700... Loss: 1.9733... Val Loss: 2.0749\n",
            "Epoch: 6/50... Step: 710... Loss: 1.9692... Val Loss: 2.0676\n",
            "Epoch: 6/50... Step: 720... Loss: 1.9714... Val Loss: 2.0616\n",
            "Epoch: 6/50... Step: 730... Loss: 1.8944... Val Loss: 2.0544\n",
            "Epoch: 6/50... Step: 740... Loss: 1.9274... Val Loss: 2.0495\n",
            "Epoch: 6/50... Step: 750... Loss: 1.9278... Val Loss: 2.0442\n",
            "Epoch: 7/50... Step: 760... Loss: 1.9800... Val Loss: 2.0414\n",
            "Epoch: 7/50... Step: 770... Loss: 1.9087... Val Loss: 2.0336\n",
            "Epoch: 7/50... Step: 780... Loss: 1.8872... Val Loss: 2.0333\n",
            "Epoch: 7/50... Step: 790... Loss: 1.9005... Val Loss: 2.0287\n",
            "Epoch: 7/50... Step: 800... Loss: 1.9115... Val Loss: 2.0243\n",
            "Epoch: 7/50... Step: 810... Loss: 1.8890... Val Loss: 2.0181\n",
            "Epoch: 7/50... Step: 820... Loss: 1.9260... Val Loss: 2.0120\n",
            "Epoch: 7/50... Step: 830... Loss: 1.8748... Val Loss: 2.0073\n",
            "Epoch: 7/50... Step: 840... Loss: 1.8520... Val Loss: 2.0033\n",
            "Epoch: 7/50... Step: 850... Loss: 1.8699... Val Loss: 2.0005\n",
            "Epoch: 7/50... Step: 860... Loss: 1.8636... Val Loss: 1.9941\n",
            "Epoch: 7/50... Step: 870... Loss: 1.8362... Val Loss: 1.9948\n",
            "Epoch: 7/50... Step: 880... Loss: 1.9120... Val Loss: 1.9868\n",
            "Epoch: 8/50... Step: 890... Loss: 1.8902... Val Loss: 1.9823\n",
            "Epoch: 8/50... Step: 900... Loss: 1.8547... Val Loss: 1.9779\n",
            "Epoch: 8/50... Step: 910... Loss: 1.8348... Val Loss: 1.9760\n",
            "Epoch: 8/50... Step: 920... Loss: 1.8037... Val Loss: 1.9730\n",
            "Epoch: 8/50... Step: 930... Loss: 1.8344... Val Loss: 1.9713\n",
            "Epoch: 8/50... Step: 940... Loss: 1.8738... Val Loss: 1.9701\n",
            "Epoch: 8/50... Step: 950... Loss: 1.8486... Val Loss: 1.9582\n",
            "Epoch: 8/50... Step: 960... Loss: 1.8021... Val Loss: 1.9594\n",
            "Epoch: 8/50... Step: 970... Loss: 1.8097... Val Loss: 1.9534\n",
            "Epoch: 8/50... Step: 980... Loss: 1.8430... Val Loss: 1.9466\n",
            "Epoch: 8/50... Step: 990... Loss: 1.8473... Val Loss: 1.9488\n",
            "Epoch: 8/50... Step: 1000... Loss: 1.8516... Val Loss: 1.9405\n",
            "Epoch: 9/50... Step: 1010... Loss: 1.8303... Val Loss: 1.9379\n",
            "Epoch: 9/50... Step: 1020... Loss: 1.7843... Val Loss: 1.9318\n",
            "Epoch: 9/50... Step: 1030... Loss: 1.7715... Val Loss: 1.9320\n",
            "Epoch: 9/50... Step: 1040... Loss: 1.7889... Val Loss: 1.9337\n",
            "Epoch: 9/50... Step: 1050... Loss: 1.7897... Val Loss: 1.9300\n",
            "Epoch: 9/50... Step: 1060... Loss: 1.7966... Val Loss: 1.9230\n",
            "Epoch: 9/50... Step: 1070... Loss: 1.7844... Val Loss: 1.9187\n",
            "Epoch: 9/50... Step: 1080... Loss: 1.8065... Val Loss: 1.9202\n",
            "Epoch: 9/50... Step: 1090... Loss: 1.7498... Val Loss: 1.9133\n",
            "Epoch: 9/50... Step: 1100... Loss: 1.7621... Val Loss: 1.9108\n",
            "Epoch: 9/50... Step: 1110... Loss: 1.7770... Val Loss: 1.9088\n",
            "Epoch: 9/50... Step: 1120... Loss: 1.7877... Val Loss: 1.9021\n",
            "Epoch: 9/50... Step: 1130... Loss: 1.7969... Val Loss: 1.9001\n",
            "Epoch: 10/50... Step: 1140... Loss: 1.7292... Val Loss: 1.9002\n",
            "Epoch: 10/50... Step: 1150... Loss: 1.7494... Val Loss: 1.8950\n",
            "Epoch: 10/50... Step: 1160... Loss: 1.7520... Val Loss: 1.8946\n",
            "Epoch: 10/50... Step: 1170... Loss: 1.7434... Val Loss: 1.8961\n",
            "Epoch: 10/50... Step: 1180... Loss: 1.7918... Val Loss: 1.8895\n",
            "Epoch: 10/50... Step: 1190... Loss: 1.7163... Val Loss: 1.8908\n",
            "Epoch: 10/50... Step: 1200... Loss: 1.7870... Val Loss: 1.8834\n",
            "Epoch: 10/50... Step: 1210... Loss: 1.7095... Val Loss: 1.8825\n",
            "Epoch: 10/50... Step: 1220... Loss: 1.7488... Val Loss: 1.8813\n",
            "Epoch: 10/50... Step: 1230... Loss: 1.7336... Val Loss: 1.8760\n",
            "Epoch: 10/50... Step: 1240... Loss: 1.7425... Val Loss: 1.8724\n",
            "Epoch: 10/50... Step: 1250... Loss: 1.7134... Val Loss: 1.8713\n",
            "Epoch: 10/50... Step: 1260... Loss: 1.7663... Val Loss: 1.8670\n",
            "Epoch: 11/50... Step: 1270... Loss: 1.7617... Val Loss: 1.8684\n",
            "Epoch: 11/50... Step: 1280... Loss: 1.7347... Val Loss: 1.8662\n",
            "Epoch: 11/50... Step: 1290... Loss: 1.7204... Val Loss: 1.8649\n",
            "Epoch: 11/50... Step: 1300... Loss: 1.7335... Val Loss: 1.8602\n",
            "Epoch: 11/50... Step: 1310... Loss: 1.6938... Val Loss: 1.8608\n",
            "Epoch: 11/50... Step: 1320... Loss: 1.7063... Val Loss: 1.8568\n",
            "Epoch: 11/50... Step: 1330... Loss: 1.7297... Val Loss: 1.8520\n",
            "Epoch: 11/50... Step: 1340... Loss: 1.7093... Val Loss: 1.8547\n",
            "Epoch: 11/50... Step: 1350... Loss: 1.7139... Val Loss: 1.8494\n",
            "Epoch: 11/50... Step: 1360... Loss: 1.6614... Val Loss: 1.8461\n",
            "Epoch: 11/50... Step: 1370... Loss: 1.6878... Val Loss: 1.8487\n",
            "Epoch: 11/50... Step: 1380... Loss: 1.6967... Val Loss: 1.8410\n",
            "Epoch: 12/50... Step: 1390... Loss: 1.7605... Val Loss: 1.8392\n",
            "Epoch: 12/50... Step: 1400... Loss: 1.6775... Val Loss: 1.8348\n",
            "Epoch: 12/50... Step: 1410... Loss: 1.6707... Val Loss: 1.8421\n",
            "Epoch: 12/50... Step: 1420... Loss: 1.6835... Val Loss: 1.8383\n",
            "Epoch: 12/50... Step: 1430... Loss: 1.7024... Val Loss: 1.8360\n",
            "Epoch: 12/50... Step: 1440... Loss: 1.6821... Val Loss: 1.8329\n",
            "Epoch: 12/50... Step: 1450... Loss: 1.7137... Val Loss: 1.8292\n",
            "Epoch: 12/50... Step: 1460... Loss: 1.6691... Val Loss: 1.8289\n",
            "Epoch: 12/50... Step: 1470... Loss: 1.6374... Val Loss: 1.8249\n",
            "Epoch: 12/50... Step: 1480... Loss: 1.6629... Val Loss: 1.8284\n",
            "Epoch: 12/50... Step: 1490... Loss: 1.6700... Val Loss: 1.8228\n",
            "Epoch: 12/50... Step: 1500... Loss: 1.6502... Val Loss: 1.8174\n",
            "Epoch: 12/50... Step: 1510... Loss: 1.7189... Val Loss: 1.8176\n",
            "Epoch: 13/50... Step: 1520... Loss: 1.6989... Val Loss: 1.8145\n",
            "Epoch: 13/50... Step: 1530... Loss: 1.6639... Val Loss: 1.8148\n",
            "Epoch: 13/50... Step: 1540... Loss: 1.6658... Val Loss: 1.8160\n",
            "Epoch: 13/50... Step: 1550... Loss: 1.6260... Val Loss: 1.8122\n",
            "Epoch: 13/50... Step: 1560... Loss: 1.6501... Val Loss: 1.8103\n",
            "Epoch: 13/50... Step: 1570... Loss: 1.7058... Val Loss: 1.8078\n",
            "Epoch: 13/50... Step: 1580... Loss: 1.6718... Val Loss: 1.8037\n",
            "Epoch: 13/50... Step: 1590... Loss: 1.6440... Val Loss: 1.8033\n",
            "Epoch: 13/50... Step: 1600... Loss: 1.6395... Val Loss: 1.8031\n",
            "Epoch: 13/50... Step: 1610... Loss: 1.6801... Val Loss: 1.7978\n",
            "Epoch: 13/50... Step: 1620... Loss: 1.6644... Val Loss: 1.7955\n",
            "Epoch: 13/50... Step: 1630... Loss: 1.6877... Val Loss: 1.7924\n",
            "Epoch: 14/50... Step: 1640... Loss: 1.6576... Val Loss: 1.7938\n",
            "Epoch: 14/50... Step: 1650... Loss: 1.6226... Val Loss: 1.7913\n",
            "Epoch: 14/50... Step: 1660... Loss: 1.6049... Val Loss: 1.7918\n",
            "Epoch: 14/50... Step: 1670... Loss: 1.6371... Val Loss: 1.7893\n",
            "Epoch: 14/50... Step: 1680... Loss: 1.6389... Val Loss: 1.7907\n",
            "Epoch: 14/50... Step: 1690... Loss: 1.6514... Val Loss: 1.7870\n",
            "Epoch: 14/50... Step: 1700... Loss: 1.6116... Val Loss: 1.7866\n",
            "Epoch: 14/50... Step: 1710... Loss: 1.6476... Val Loss: 1.7809\n",
            "Epoch: 14/50... Step: 1720... Loss: 1.5992... Val Loss: 1.7828\n",
            "Epoch: 14/50... Step: 1730... Loss: 1.6176... Val Loss: 1.7806\n",
            "Epoch: 14/50... Step: 1740... Loss: 1.6142... Val Loss: 1.7811\n",
            "Epoch: 14/50... Step: 1750... Loss: 1.6632... Val Loss: 1.7731\n",
            "Epoch: 14/50... Step: 1760... Loss: 1.6453... Val Loss: 1.7732\n",
            "Epoch: 15/50... Step: 1770... Loss: 1.5953... Val Loss: 1.7687\n",
            "Epoch: 15/50... Step: 1780... Loss: 1.6124... Val Loss: 1.7712\n",
            "Epoch: 15/50... Step: 1790... Loss: 1.6189... Val Loss: 1.7727\n",
            "Epoch: 15/50... Step: 1800... Loss: 1.6130... Val Loss: 1.7733\n",
            "Epoch: 15/50... Step: 1810... Loss: 1.6589... Val Loss: 1.7676\n",
            "Epoch: 15/50... Step: 1820... Loss: 1.5875... Val Loss: 1.7717\n",
            "Epoch: 15/50... Step: 1830... Loss: 1.6475... Val Loss: 1.7633\n",
            "Epoch: 15/50... Step: 1840... Loss: 1.5772... Val Loss: 1.7628\n",
            "Epoch: 15/50... Step: 1850... Loss: 1.6105... Val Loss: 1.7628\n",
            "Epoch: 15/50... Step: 1860... Loss: 1.6026... Val Loss: 1.7657\n",
            "Epoch: 15/50... Step: 1870... Loss: 1.6312... Val Loss: 1.7579\n",
            "Epoch: 15/50... Step: 1880... Loss: 1.5909... Val Loss: 1.7572\n",
            "Epoch: 15/50... Step: 1890... Loss: 1.6538... Val Loss: 1.7534\n",
            "Epoch: 16/50... Step: 1900... Loss: 1.6205... Val Loss: 1.7541\n",
            "Epoch: 16/50... Step: 1910... Loss: 1.6030... Val Loss: 1.7548\n",
            "Epoch: 16/50... Step: 1920... Loss: 1.6023... Val Loss: 1.7541\n",
            "Epoch: 16/50... Step: 1930... Loss: 1.6027... Val Loss: 1.7514\n",
            "Epoch: 16/50... Step: 1940... Loss: 1.5828... Val Loss: 1.7513\n",
            "Epoch: 16/50... Step: 1950... Loss: 1.5804... Val Loss: 1.7498\n",
            "Epoch: 16/50... Step: 1960... Loss: 1.6113... Val Loss: 1.7449\n",
            "Epoch: 16/50... Step: 1970... Loss: 1.5807... Val Loss: 1.7434\n",
            "Epoch: 16/50... Step: 1980... Loss: 1.5803... Val Loss: 1.7462\n",
            "Epoch: 16/50... Step: 1990... Loss: 1.5602... Val Loss: 1.7391\n",
            "Epoch: 16/50... Step: 2000... Loss: 1.5887... Val Loss: 1.7433\n",
            "Epoch: 16/50... Step: 2010... Loss: 1.5803... Val Loss: 1.7363\n",
            "Epoch: 17/50... Step: 2020... Loss: 1.6431... Val Loss: 1.7368\n",
            "Epoch: 17/50... Step: 2030... Loss: 1.5531... Val Loss: 1.7347\n",
            "Epoch: 17/50... Step: 2040... Loss: 1.5478... Val Loss: 1.7379\n",
            "Epoch: 17/50... Step: 2050... Loss: 1.5839... Val Loss: 1.7361\n",
            "Epoch: 17/50... Step: 2060... Loss: 1.5921... Val Loss: 1.7408\n",
            "Epoch: 17/50... Step: 2070... Loss: 1.5611... Val Loss: 1.7325\n",
            "Epoch: 17/50... Step: 2080... Loss: 1.5883... Val Loss: 1.7342\n",
            "Epoch: 17/50... Step: 2090... Loss: 1.5614... Val Loss: 1.7295\n",
            "Epoch: 17/50... Step: 2100... Loss: 1.5334... Val Loss: 1.7303\n",
            "Epoch: 17/50... Step: 2110... Loss: 1.5424... Val Loss: 1.7287\n",
            "Epoch: 17/50... Step: 2120... Loss: 1.5618... Val Loss: 1.7280\n",
            "Epoch: 17/50... Step: 2130... Loss: 1.5504... Val Loss: 1.7222\n",
            "Epoch: 17/50... Step: 2140... Loss: 1.5991... Val Loss: 1.7237\n",
            "Epoch: 18/50... Step: 2150... Loss: 1.5803... Val Loss: 1.7241\n",
            "Epoch: 18/50... Step: 2160... Loss: 1.5431... Val Loss: 1.7251\n",
            "Epoch: 18/50... Step: 2170... Loss: 1.5579... Val Loss: 1.7270\n",
            "Epoch: 18/50... Step: 2180... Loss: 1.5158... Val Loss: 1.7256\n",
            "Epoch: 18/50... Step: 2190... Loss: 1.5315... Val Loss: 1.7203\n",
            "Epoch: 18/50... Step: 2200... Loss: 1.5938... Val Loss: 1.7197\n",
            "Epoch: 18/50... Step: 2210... Loss: 1.5764... Val Loss: 1.7131\n",
            "Epoch: 18/50... Step: 2220... Loss: 1.5352... Val Loss: 1.7131\n",
            "Epoch: 18/50... Step: 2230... Loss: 1.5406... Val Loss: 1.7148\n",
            "Epoch: 18/50... Step: 2240... Loss: 1.5717... Val Loss: 1.7112\n",
            "Epoch: 18/50... Step: 2250... Loss: 1.5714... Val Loss: 1.7106\n",
            "Epoch: 18/50... Step: 2260... Loss: 1.5989... Val Loss: 1.7095\n",
            "Epoch: 19/50... Step: 2270... Loss: 1.5544... Val Loss: 1.7069\n",
            "Epoch: 19/50... Step: 2280... Loss: 1.5269... Val Loss: 1.7081\n",
            "Epoch: 19/50... Step: 2290... Loss: 1.5018... Val Loss: 1.7105\n",
            "Epoch: 19/50... Step: 2300... Loss: 1.5337... Val Loss: 1.7077\n",
            "Epoch: 19/50... Step: 2310... Loss: 1.5517... Val Loss: 1.7069\n",
            "Epoch: 19/50... Step: 2320... Loss: 1.5501... Val Loss: 1.7039\n",
            "Epoch: 19/50... Step: 2330... Loss: 1.5339... Val Loss: 1.7047\n",
            "Epoch: 19/50... Step: 2340... Loss: 1.5619... Val Loss: 1.7006\n",
            "Epoch: 19/50... Step: 2350... Loss: 1.5024... Val Loss: 1.7006\n",
            "Epoch: 19/50... Step: 2360... Loss: 1.5214... Val Loss: 1.7003\n",
            "Epoch: 19/50... Step: 2370... Loss: 1.5119... Val Loss: 1.6975\n",
            "Epoch: 19/50... Step: 2380... Loss: 1.5622... Val Loss: 1.6932\n",
            "Epoch: 19/50... Step: 2390... Loss: 1.5500... Val Loss: 1.6918\n",
            "Epoch: 20/50... Step: 2400... Loss: 1.4986... Val Loss: 1.6934\n",
            "Epoch: 20/50... Step: 2410... Loss: 1.5119... Val Loss: 1.6926\n",
            "Epoch: 20/50... Step: 2420... Loss: 1.5184... Val Loss: 1.6951\n",
            "Epoch: 20/50... Step: 2430... Loss: 1.5113... Val Loss: 1.6953\n",
            "Epoch: 20/50... Step: 2440... Loss: 1.5659... Val Loss: 1.6926\n",
            "Epoch: 20/50... Step: 2450... Loss: 1.4785... Val Loss: 1.6936\n",
            "Epoch: 20/50... Step: 2460... Loss: 1.5401... Val Loss: 1.6876\n",
            "Epoch: 20/50... Step: 2470... Loss: 1.4845... Val Loss: 1.6887\n",
            "Epoch: 20/50... Step: 2480... Loss: 1.5158... Val Loss: 1.6849\n",
            "Epoch: 20/50... Step: 2490... Loss: 1.5053... Val Loss: 1.6832\n",
            "Epoch: 20/50... Step: 2500... Loss: 1.5322... Val Loss: 1.6824\n",
            "Epoch: 20/50... Step: 2510... Loss: 1.4917... Val Loss: 1.6801\n",
            "Epoch: 20/50... Step: 2520... Loss: 1.5642... Val Loss: 1.6792\n",
            "Epoch: 21/50... Step: 2530... Loss: 1.5139... Val Loss: 1.6793\n",
            "Epoch: 21/50... Step: 2540... Loss: 1.5076... Val Loss: 1.6816\n",
            "Epoch: 21/50... Step: 2550... Loss: 1.5110... Val Loss: 1.6849\n",
            "Epoch: 21/50... Step: 2560... Loss: 1.5153... Val Loss: 1.6774\n",
            "Epoch: 21/50... Step: 2570... Loss: 1.4948... Val Loss: 1.6784\n",
            "Epoch: 21/50... Step: 2580... Loss: 1.4969... Val Loss: 1.6765\n",
            "Epoch: 21/50... Step: 2590... Loss: 1.5261... Val Loss: 1.6719\n",
            "Epoch: 21/50... Step: 2600... Loss: 1.4806... Val Loss: 1.6714\n",
            "Epoch: 21/50... Step: 2610... Loss: 1.5002... Val Loss: 1.6735\n",
            "Epoch: 21/50... Step: 2620... Loss: 1.4797... Val Loss: 1.6683\n",
            "Epoch: 21/50... Step: 2630... Loss: 1.4927... Val Loss: 1.6688\n",
            "Epoch: 21/50... Step: 2640... Loss: 1.4985... Val Loss: 1.6643\n",
            "Epoch: 22/50... Step: 2650... Loss: 1.5448... Val Loss: 1.6628\n",
            "Epoch: 22/50... Step: 2660... Loss: 1.4765... Val Loss: 1.6689\n",
            "Epoch: 22/50... Step: 2670... Loss: 1.4626... Val Loss: 1.6665\n",
            "Epoch: 22/50... Step: 2680... Loss: 1.4906... Val Loss: 1.6641\n",
            "Epoch: 22/50... Step: 2690... Loss: 1.5087... Val Loss: 1.6680\n",
            "Epoch: 22/50... Step: 2700... Loss: 1.4874... Val Loss: 1.6634\n",
            "Epoch: 22/50... Step: 2710... Loss: 1.5047... Val Loss: 1.6634\n",
            "Epoch: 22/50... Step: 2720... Loss: 1.4777... Val Loss: 1.6586\n",
            "Epoch: 22/50... Step: 2730... Loss: 1.4450... Val Loss: 1.6608\n",
            "Epoch: 22/50... Step: 2740... Loss: 1.4533... Val Loss: 1.6568\n",
            "Epoch: 22/50... Step: 2750... Loss: 1.4711... Val Loss: 1.6597\n",
            "Epoch: 22/50... Step: 2760... Loss: 1.4495... Val Loss: 1.6526\n",
            "Epoch: 22/50... Step: 2770... Loss: 1.5053... Val Loss: 1.6526\n",
            "Epoch: 23/50... Step: 2780... Loss: 1.4952... Val Loss: 1.6557\n",
            "Epoch: 23/50... Step: 2790... Loss: 1.4676... Val Loss: 1.6565\n",
            "Epoch: 23/50... Step: 2800... Loss: 1.4755... Val Loss: 1.6588\n",
            "Epoch: 23/50... Step: 2810... Loss: 1.4322... Val Loss: 1.6624\n",
            "Epoch: 23/50... Step: 2820... Loss: 1.4591... Val Loss: 1.6521\n",
            "Epoch: 23/50... Step: 2830... Loss: 1.4938... Val Loss: 1.6506\n",
            "Epoch: 23/50... Step: 2840... Loss: 1.4909... Val Loss: 1.6501\n",
            "Epoch: 23/50... Step: 2850... Loss: 1.4665... Val Loss: 1.6486\n",
            "Epoch: 23/50... Step: 2860... Loss: 1.4486... Val Loss: 1.6494\n",
            "Epoch: 23/50... Step: 2870... Loss: 1.4761... Val Loss: 1.6426\n",
            "Epoch: 23/50... Step: 2880... Loss: 1.4791... Val Loss: 1.6425\n",
            "Epoch: 23/50... Step: 2890... Loss: 1.5072... Val Loss: 1.6457\n",
            "Epoch: 24/50... Step: 2900... Loss: 1.4658... Val Loss: 1.6394\n",
            "Epoch: 24/50... Step: 2910... Loss: 1.4480... Val Loss: 1.6380\n",
            "Epoch: 24/50... Step: 2920... Loss: 1.4292... Val Loss: 1.6489\n",
            "Epoch: 24/50... Step: 2930... Loss: 1.4628... Val Loss: 1.6449\n",
            "Epoch: 24/50... Step: 2940... Loss: 1.4815... Val Loss: 1.6387\n",
            "Epoch: 24/50... Step: 2950... Loss: 1.4718... Val Loss: 1.6393\n",
            "Epoch: 24/50... Step: 2960... Loss: 1.4662... Val Loss: 1.6405\n",
            "Epoch: 24/50... Step: 2970... Loss: 1.4752... Val Loss: 1.6341\n",
            "Epoch: 24/50... Step: 2980... Loss: 1.4259... Val Loss: 1.6363\n",
            "Epoch: 24/50... Step: 2990... Loss: 1.4520... Val Loss: 1.6368\n",
            "Epoch: 24/50... Step: 3000... Loss: 1.4371... Val Loss: 1.6308\n",
            "Epoch: 24/50... Step: 3010... Loss: 1.4952... Val Loss: 1.6292\n",
            "Epoch: 24/50... Step: 3020... Loss: 1.4635... Val Loss: 1.6282\n",
            "Epoch: 25/50... Step: 3030... Loss: 1.4216... Val Loss: 1.6286\n",
            "Epoch: 25/50... Step: 3040... Loss: 1.4353... Val Loss: 1.6364\n",
            "Epoch: 25/50... Step: 3050... Loss: 1.4422... Val Loss: 1.6344\n",
            "Epoch: 25/50... Step: 3060... Loss: 1.4445... Val Loss: 1.6323\n",
            "Epoch: 25/50... Step: 3070... Loss: 1.4732... Val Loss: 1.6360\n",
            "Epoch: 25/50... Step: 3080... Loss: 1.4135... Val Loss: 1.6332\n",
            "Epoch: 25/50... Step: 3090... Loss: 1.4646... Val Loss: 1.6259\n",
            "Epoch: 25/50... Step: 3100... Loss: 1.4175... Val Loss: 1.6294\n",
            "Epoch: 25/50... Step: 3110... Loss: 1.4554... Val Loss: 1.6291\n",
            "Epoch: 25/50... Step: 3120... Loss: 1.4229... Val Loss: 1.6262\n",
            "Epoch: 25/50... Step: 3130... Loss: 1.4517... Val Loss: 1.6219\n",
            "Epoch: 25/50... Step: 3140... Loss: 1.4333... Val Loss: 1.6220\n",
            "Epoch: 25/50... Step: 3150... Loss: 1.5026... Val Loss: 1.6228\n",
            "Epoch: 26/50... Step: 3160... Loss: 1.4475... Val Loss: 1.6205\n",
            "Epoch: 26/50... Step: 3170... Loss: 1.4214... Val Loss: 1.6239\n",
            "Epoch: 26/50... Step: 3180... Loss: 1.4501... Val Loss: 1.6276\n",
            "Epoch: 26/50... Step: 3190... Loss: 1.4656... Val Loss: 1.6207\n",
            "Epoch: 26/50... Step: 3200... Loss: 1.4254... Val Loss: 1.6183\n",
            "Epoch: 26/50... Step: 3210... Loss: 1.4237... Val Loss: 1.6194\n",
            "Epoch: 26/50... Step: 3220... Loss: 1.4489... Val Loss: 1.6196\n",
            "Epoch: 26/50... Step: 3230... Loss: 1.4266... Val Loss: 1.6179\n",
            "Epoch: 26/50... Step: 3240... Loss: 1.4197... Val Loss: 1.6142\n",
            "Epoch: 26/50... Step: 3250... Loss: 1.4113... Val Loss: 1.6132\n",
            "Epoch: 26/50... Step: 3260... Loss: 1.4229... Val Loss: 1.6155\n",
            "Epoch: 26/50... Step: 3270... Loss: 1.4100... Val Loss: 1.6119\n",
            "Epoch: 27/50... Step: 3280... Loss: 1.4739... Val Loss: 1.6129\n",
            "Epoch: 27/50... Step: 3290... Loss: 1.4075... Val Loss: 1.6113\n",
            "Epoch: 27/50... Step: 3300... Loss: 1.3962... Val Loss: 1.6183\n",
            "Epoch: 27/50... Step: 3310... Loss: 1.4241... Val Loss: 1.6131\n",
            "Epoch: 27/50... Step: 3320... Loss: 1.4417... Val Loss: 1.6127\n",
            "Epoch: 27/50... Step: 3330... Loss: 1.4285... Val Loss: 1.6122\n",
            "Epoch: 27/50... Step: 3340... Loss: 1.4481... Val Loss: 1.6155\n",
            "Epoch: 27/50... Step: 3350... Loss: 1.4147... Val Loss: 1.6050\n",
            "Epoch: 27/50... Step: 3360... Loss: 1.3741... Val Loss: 1.6112\n",
            "Epoch: 27/50... Step: 3370... Loss: 1.3998... Val Loss: 1.6106\n",
            "Epoch: 27/50... Step: 3380... Loss: 1.3964... Val Loss: 1.6137\n",
            "Epoch: 27/50... Step: 3390... Loss: 1.3799... Val Loss: 1.6043\n",
            "Epoch: 27/50... Step: 3400... Loss: 1.4275... Val Loss: 1.6072\n",
            "Epoch: 28/50... Step: 3410... Loss: 1.4285... Val Loss: 1.6110\n",
            "Epoch: 28/50... Step: 3420... Loss: 1.4007... Val Loss: 1.6147\n",
            "Epoch: 28/50... Step: 3430... Loss: 1.4107... Val Loss: 1.6147\n",
            "Epoch: 28/50... Step: 3440... Loss: 1.3762... Val Loss: 1.6119\n",
            "Epoch: 28/50... Step: 3450... Loss: 1.3914... Val Loss: 1.6157\n",
            "Epoch: 28/50... Step: 3460... Loss: 1.4311... Val Loss: 1.6066\n",
            "Epoch: 28/50... Step: 3470... Loss: 1.4299... Val Loss: 1.6012\n",
            "Epoch: 28/50... Step: 3480... Loss: 1.4050... Val Loss: 1.6016\n",
            "Epoch: 28/50... Step: 3490... Loss: 1.3934... Val Loss: 1.6089\n",
            "Epoch: 28/50... Step: 3500... Loss: 1.3993... Val Loss: 1.5990\n",
            "Epoch: 28/50... Step: 3510... Loss: 1.4198... Val Loss: 1.5979\n",
            "Epoch: 28/50... Step: 3520... Loss: 1.4340... Val Loss: 1.6007\n",
            "Epoch: 29/50... Step: 3530... Loss: 1.4138... Val Loss: 1.6001\n",
            "Epoch: 29/50... Step: 3540... Loss: 1.3833... Val Loss: 1.5963\n",
            "Epoch: 29/50... Step: 3550... Loss: 1.3723... Val Loss: 1.6019\n",
            "Epoch: 29/50... Step: 3560... Loss: 1.4087... Val Loss: 1.6070\n",
            "Epoch: 29/50... Step: 3570... Loss: 1.4326... Val Loss: 1.5995\n",
            "Epoch: 29/50... Step: 3580... Loss: 1.4333... Val Loss: 1.5978\n",
            "Epoch: 29/50... Step: 3590... Loss: 1.4089... Val Loss: 1.5982\n",
            "Epoch: 29/50... Step: 3600... Loss: 1.4193... Val Loss: 1.5972\n",
            "Epoch: 29/50... Step: 3610... Loss: 1.3630... Val Loss: 1.5970\n",
            "Epoch: 29/50... Step: 3620... Loss: 1.3928... Val Loss: 1.5932\n",
            "Epoch: 29/50... Step: 3630... Loss: 1.3611... Val Loss: 1.5934\n",
            "Epoch: 29/50... Step: 3640... Loss: 1.4285... Val Loss: 1.5938\n",
            "Epoch: 29/50... Step: 3650... Loss: 1.4004... Val Loss: 1.5958\n",
            "Epoch: 30/50... Step: 3660... Loss: 1.3694... Val Loss: 1.5915\n",
            "Epoch: 30/50... Step: 3670... Loss: 1.3788... Val Loss: 1.5979\n",
            "Epoch: 30/50... Step: 3680... Loss: 1.3872... Val Loss: 1.5994\n",
            "Epoch: 30/50... Step: 3690... Loss: 1.3910... Val Loss: 1.5954\n",
            "Epoch: 30/50... Step: 3700... Loss: 1.4321... Val Loss: 1.5949\n",
            "Epoch: 30/50... Step: 3710... Loss: 1.3529... Val Loss: 1.6030\n",
            "Epoch: 30/50... Step: 3720... Loss: 1.3984... Val Loss: 1.5921\n",
            "Epoch: 30/50... Step: 3730... Loss: 1.3756... Val Loss: 1.5871\n",
            "Epoch: 30/50... Step: 3740... Loss: 1.4028... Val Loss: 1.5918\n",
            "Epoch: 30/50... Step: 3750... Loss: 1.3736... Val Loss: 1.5917\n",
            "Epoch: 30/50... Step: 3760... Loss: 1.3838... Val Loss: 1.5912\n",
            "Epoch: 30/50... Step: 3770... Loss: 1.3729... Val Loss: 1.5894\n",
            "Epoch: 30/50... Step: 3780... Loss: 1.4352... Val Loss: 1.5902\n",
            "Epoch: 31/50... Step: 3790... Loss: 1.3819... Val Loss: 1.5903\n",
            "Epoch: 31/50... Step: 3800... Loss: 1.3897... Val Loss: 1.5923\n",
            "Epoch: 31/50... Step: 3810... Loss: 1.3973... Val Loss: 1.5971\n",
            "Epoch: 31/50... Step: 3820... Loss: 1.4065... Val Loss: 1.5918\n",
            "Epoch: 31/50... Step: 3830... Loss: 1.3836... Val Loss: 1.5954\n",
            "Epoch: 31/50... Step: 3840... Loss: 1.3668... Val Loss: 1.5918\n",
            "Epoch: 31/50... Step: 3850... Loss: 1.3935... Val Loss: 1.5869\n",
            "Epoch: 31/50... Step: 3860... Loss: 1.3783... Val Loss: 1.5902\n",
            "Epoch: 31/50... Step: 3870... Loss: 1.3777... Val Loss: 1.5882\n",
            "Epoch: 31/50... Step: 3880... Loss: 1.3645... Val Loss: 1.5841\n",
            "Epoch: 31/50... Step: 3890... Loss: 1.3697... Val Loss: 1.5816\n",
            "Epoch: 31/50... Step: 3900... Loss: 1.3568... Val Loss: 1.5874\n",
            "Epoch: 32/50... Step: 3910... Loss: 1.4096... Val Loss: 1.5821\n",
            "Epoch: 32/50... Step: 3920... Loss: 1.3671... Val Loss: 1.5858\n",
            "Epoch: 32/50... Step: 3930... Loss: 1.3600... Val Loss: 1.5878\n",
            "Epoch: 32/50... Step: 3940... Loss: 1.3746... Val Loss: 1.5870\n",
            "Epoch: 32/50... Step: 3950... Loss: 1.3951... Val Loss: 1.5826\n",
            "Epoch: 32/50... Step: 3960... Loss: 1.3825... Val Loss: 1.5858\n",
            "Epoch: 32/50... Step: 3970... Loss: 1.3887... Val Loss: 1.5930\n",
            "Epoch: 32/50... Step: 3980... Loss: 1.3703... Val Loss: 1.5824\n",
            "Epoch: 32/50... Step: 3990... Loss: 1.3380... Val Loss: 1.5845\n",
            "Epoch: 32/50... Step: 4000... Loss: 1.3422... Val Loss: 1.5814\n",
            "Epoch: 32/50... Step: 4010... Loss: 1.3609... Val Loss: 1.5824\n",
            "Epoch: 32/50... Step: 4020... Loss: 1.3395... Val Loss: 1.5781\n",
            "Epoch: 32/50... Step: 4030... Loss: 1.3708... Val Loss: 1.5814\n",
            "Epoch: 33/50... Step: 4040... Loss: 1.3764... Val Loss: 1.5835\n",
            "Epoch: 33/50... Step: 4050... Loss: 1.3567... Val Loss: 1.5853\n",
            "Epoch: 33/50... Step: 4060... Loss: 1.3687... Val Loss: 1.5851\n",
            "Epoch: 33/50... Step: 4070... Loss: 1.3329... Val Loss: 1.5799\n",
            "Epoch: 33/50... Step: 4080... Loss: 1.3718... Val Loss: 1.5869\n",
            "Epoch: 33/50... Step: 4090... Loss: 1.3954... Val Loss: 1.5889\n",
            "Epoch: 33/50... Step: 4100... Loss: 1.3874... Val Loss: 1.5788\n",
            "Epoch: 33/50... Step: 4110... Loss: 1.3571... Val Loss: 1.5745\n",
            "Epoch: 33/50... Step: 4120... Loss: 1.3547... Val Loss: 1.5789\n",
            "Epoch: 33/50... Step: 4130... Loss: 1.3604... Val Loss: 1.5792\n",
            "Epoch: 33/50... Step: 4140... Loss: 1.3663... Val Loss: 1.5759\n",
            "Epoch: 33/50... Step: 4150... Loss: 1.3848... Val Loss: 1.5802\n",
            "Epoch: 34/50... Step: 4160... Loss: 1.3771... Val Loss: 1.5770\n",
            "Epoch: 34/50... Step: 4170... Loss: 1.3352... Val Loss: 1.5760\n",
            "Epoch: 34/50... Step: 4180... Loss: 1.3361... Val Loss: 1.5773\n",
            "Epoch: 34/50... Step: 4190... Loss: 1.3597... Val Loss: 1.5815\n",
            "Epoch: 34/50... Step: 4200... Loss: 1.3723... Val Loss: 1.5762\n",
            "Epoch: 34/50... Step: 4210... Loss: 1.3729... Val Loss: 1.5761\n",
            "Epoch: 34/50... Step: 4220... Loss: 1.3695... Val Loss: 1.5754\n",
            "Epoch: 34/50... Step: 4230... Loss: 1.3715... Val Loss: 1.5769\n",
            "Epoch: 34/50... Step: 4240... Loss: 1.3300... Val Loss: 1.5835\n",
            "Epoch: 34/50... Step: 4250... Loss: 1.3491... Val Loss: 1.5786\n",
            "Epoch: 34/50... Step: 4260... Loss: 1.3234... Val Loss: 1.5738\n",
            "Epoch: 34/50... Step: 4270... Loss: 1.3848... Val Loss: 1.5713\n",
            "Epoch: 34/50... Step: 4280... Loss: 1.3566... Val Loss: 1.5757\n",
            "Epoch: 35/50... Step: 4290... Loss: 1.3281... Val Loss: 1.5731\n",
            "Epoch: 35/50... Step: 4300... Loss: 1.3354... Val Loss: 1.5767\n",
            "Epoch: 35/50... Step: 4310... Loss: 1.3493... Val Loss: 1.5790\n",
            "Epoch: 35/50... Step: 4320... Loss: 1.3575... Val Loss: 1.5721\n",
            "Epoch: 35/50... Step: 4330... Loss: 1.3887... Val Loss: 1.5717\n",
            "Epoch: 35/50... Step: 4340... Loss: 1.3124... Val Loss: 1.5790\n",
            "Epoch: 35/50... Step: 4350... Loss: 1.3555... Val Loss: 1.5788\n",
            "Epoch: 35/50... Step: 4360... Loss: 1.3440... Val Loss: 1.5714\n",
            "Epoch: 35/50... Step: 4370... Loss: 1.3527... Val Loss: 1.5739\n",
            "Epoch: 35/50... Step: 4380... Loss: 1.3333... Val Loss: 1.5734\n",
            "Epoch: 35/50... Step: 4390... Loss: 1.3444... Val Loss: 1.5727\n",
            "Epoch: 35/50... Step: 4400... Loss: 1.3416... Val Loss: 1.5737\n",
            "Epoch: 35/50... Step: 4410... Loss: 1.3988... Val Loss: 1.5738\n",
            "Epoch: 36/50... Step: 4420... Loss: 1.3485... Val Loss: 1.5763\n",
            "Epoch: 36/50... Step: 4430... Loss: 1.3468... Val Loss: 1.5729\n",
            "Epoch: 36/50... Step: 4440... Loss: 1.3497... Val Loss: 1.5763\n",
            "Epoch: 36/50... Step: 4450... Loss: 1.3590... Val Loss: 1.5738\n",
            "Epoch: 36/50... Step: 4460... Loss: 1.3363... Val Loss: 1.5748\n",
            "Epoch: 36/50... Step: 4470... Loss: 1.3171... Val Loss: 1.5703\n",
            "Epoch: 36/50... Step: 4480... Loss: 1.3701... Val Loss: 1.5692\n",
            "Epoch: 36/50... Step: 4490... Loss: 1.3417... Val Loss: 1.5676\n",
            "Epoch: 36/50... Step: 4500... Loss: 1.3367... Val Loss: 1.5677\n",
            "Epoch: 36/50... Step: 4510... Loss: 1.3261... Val Loss: 1.5684\n",
            "Epoch: 36/50... Step: 4520... Loss: 1.3344... Val Loss: 1.5652\n",
            "Epoch: 36/50... Step: 4530... Loss: 1.3142... Val Loss: 1.5727\n",
            "Epoch: 37/50... Step: 4540... Loss: 1.3629... Val Loss: 1.5653\n",
            "Epoch: 37/50... Step: 4550... Loss: 1.3192... Val Loss: 1.5658\n",
            "Epoch: 37/50... Step: 4560... Loss: 1.3247... Val Loss: 1.5689\n",
            "Epoch: 37/50... Step: 4570... Loss: 1.3327... Val Loss: 1.5700\n",
            "Epoch: 37/50... Step: 4580... Loss: 1.3565... Val Loss: 1.5659\n",
            "Epoch: 37/50... Step: 4590... Loss: 1.3528... Val Loss: 1.5663\n",
            "Epoch: 37/50... Step: 4600... Loss: 1.3505... Val Loss: 1.5711\n",
            "Epoch: 37/50... Step: 4610... Loss: 1.3395... Val Loss: 1.5654\n",
            "Epoch: 37/50... Step: 4620... Loss: 1.2983... Val Loss: 1.5721\n",
            "Epoch: 37/50... Step: 4630... Loss: 1.3054... Val Loss: 1.5685\n",
            "Epoch: 37/50... Step: 4640... Loss: 1.3192... Val Loss: 1.5685\n",
            "Epoch: 37/50... Step: 4650... Loss: 1.2950... Val Loss: 1.5659\n",
            "Epoch: 37/50... Step: 4660... Loss: 1.3225... Val Loss: 1.5709\n",
            "Epoch: 38/50... Step: 4670... Loss: 1.3474... Val Loss: 1.5719\n",
            "Epoch: 38/50... Step: 4680... Loss: 1.3304... Val Loss: 1.5741\n",
            "Epoch: 38/50... Step: 4690... Loss: 1.3243... Val Loss: 1.5748\n",
            "Epoch: 38/50... Step: 4700... Loss: 1.3062... Val Loss: 1.5693\n",
            "Epoch: 38/50... Step: 4710... Loss: 1.3289... Val Loss: 1.5686\n",
            "Epoch: 38/50... Step: 4720... Loss: 1.3540... Val Loss: 1.5700\n",
            "Epoch: 38/50... Step: 4730... Loss: 1.3550... Val Loss: 1.5651\n",
            "Epoch: 38/50... Step: 4740... Loss: 1.3254... Val Loss: 1.5614\n",
            "Epoch: 38/50... Step: 4750... Loss: 1.3158... Val Loss: 1.5640\n",
            "Epoch: 38/50... Step: 4760... Loss: 1.3256... Val Loss: 1.5650\n",
            "Epoch: 38/50... Step: 4770... Loss: 1.3124... Val Loss: 1.5650\n",
            "Epoch: 38/50... Step: 4780... Loss: 1.3334... Val Loss: 1.5660\n",
            "Epoch: 39/50... Step: 4790... Loss: 1.3508... Val Loss: 1.5647\n",
            "Epoch: 39/50... Step: 4800... Loss: 1.3130... Val Loss: 1.5664\n",
            "Epoch: 39/50... Step: 4810... Loss: 1.3071... Val Loss: 1.5683\n",
            "Epoch: 39/50... Step: 4820... Loss: 1.3325... Val Loss: 1.5679\n",
            "Epoch: 39/50... Step: 4830... Loss: 1.3430... Val Loss: 1.5628\n",
            "Epoch: 39/50... Step: 4840... Loss: 1.3517... Val Loss: 1.5647\n",
            "Epoch: 39/50... Step: 4850... Loss: 1.3252... Val Loss: 1.5609\n",
            "Epoch: 39/50... Step: 4860... Loss: 1.3424... Val Loss: 1.5612\n",
            "Epoch: 39/50... Step: 4870... Loss: 1.2865... Val Loss: 1.5638\n",
            "Epoch: 39/50... Step: 4880... Loss: 1.3151... Val Loss: 1.5626\n",
            "Epoch: 39/50... Step: 4890... Loss: 1.2847... Val Loss: 1.5634\n",
            "Epoch: 39/50... Step: 4900... Loss: 1.3605... Val Loss: 1.5621\n",
            "Epoch: 39/50... Step: 4910... Loss: 1.3082... Val Loss: 1.5655\n",
            "Epoch: 40/50... Step: 4920... Loss: 1.2885... Val Loss: 1.5610\n",
            "Epoch: 40/50... Step: 4930... Loss: 1.3074... Val Loss: 1.5647\n",
            "Epoch: 40/50... Step: 4940... Loss: 1.3162... Val Loss: 1.5678\n",
            "Epoch: 40/50... Step: 4950... Loss: 1.3094... Val Loss: 1.5607\n",
            "Epoch: 40/50... Step: 4960... Loss: 1.3523... Val Loss: 1.5623\n",
            "Epoch: 40/50... Step: 4970... Loss: 1.2732... Val Loss: 1.5670\n",
            "Epoch: 40/50... Step: 4980... Loss: 1.3304... Val Loss: 1.5654\n",
            "Epoch: 40/50... Step: 4990... Loss: 1.3041... Val Loss: 1.5601\n",
            "Epoch: 40/50... Step: 5000... Loss: 1.3191... Val Loss: 1.5635\n",
            "Epoch: 40/50... Step: 5010... Loss: 1.2990... Val Loss: 1.5634\n",
            "Epoch: 40/50... Step: 5020... Loss: 1.3066... Val Loss: 1.5629\n",
            "Epoch: 40/50... Step: 5030... Loss: 1.3026... Val Loss: 1.5640\n",
            "Epoch: 40/50... Step: 5040... Loss: 1.3633... Val Loss: 1.5668\n",
            "Epoch: 41/50... Step: 5050... Loss: 1.3175... Val Loss: 1.5648\n",
            "Epoch: 41/50... Step: 5060... Loss: 1.3160... Val Loss: 1.5634\n",
            "Epoch: 41/50... Step: 5070... Loss: 1.3227... Val Loss: 1.5650\n",
            "Epoch: 41/50... Step: 5080... Loss: 1.3252... Val Loss: 1.5606\n",
            "Epoch: 41/50... Step: 5090... Loss: 1.3019... Val Loss: 1.5673\n",
            "Epoch: 41/50... Step: 5100... Loss: 1.3024... Val Loss: 1.5624\n",
            "Epoch: 41/50... Step: 5110... Loss: 1.3245... Val Loss: 1.5579\n",
            "Epoch: 41/50... Step: 5120... Loss: 1.3146... Val Loss: 1.5577\n",
            "Epoch: 41/50... Step: 5130... Loss: 1.3167... Val Loss: 1.5587\n",
            "Epoch: 41/50... Step: 5140... Loss: 1.3010... Val Loss: 1.5617\n",
            "Epoch: 41/50... Step: 5150... Loss: 1.2984... Val Loss: 1.5570\n",
            "Epoch: 41/50... Step: 5160... Loss: 1.2783... Val Loss: 1.5624\n",
            "Epoch: 42/50... Step: 5170... Loss: 1.3384... Val Loss: 1.5578\n",
            "Epoch: 42/50... Step: 5180... Loss: 1.2999... Val Loss: 1.5598\n",
            "Epoch: 42/50... Step: 5190... Loss: 1.3001... Val Loss: 1.5591\n",
            "Epoch: 42/50... Step: 5200... Loss: 1.3166... Val Loss: 1.5602\n",
            "Epoch: 42/50... Step: 5210... Loss: 1.3328... Val Loss: 1.5562\n",
            "Epoch: 42/50... Step: 5220... Loss: 1.3113... Val Loss: 1.5566\n",
            "Epoch: 42/50... Step: 5230... Loss: 1.3216... Val Loss: 1.5577\n",
            "Epoch: 42/50... Step: 5240... Loss: 1.3024... Val Loss: 1.5552\n",
            "Epoch: 42/50... Step: 5250... Loss: 1.2705... Val Loss: 1.5634\n",
            "Epoch: 42/50... Step: 5260... Loss: 1.2698... Val Loss: 1.5572\n",
            "Epoch: 42/50... Step: 5270... Loss: 1.2966... Val Loss: 1.5592\n",
            "Epoch: 42/50... Step: 5280... Loss: 1.2667... Val Loss: 1.5560\n",
            "Epoch: 42/50... Step: 5290... Loss: 1.2924... Val Loss: 1.5638\n",
            "Epoch: 43/50... Step: 5300... Loss: 1.3110... Val Loss: 1.5589\n",
            "Epoch: 43/50... Step: 5310... Loss: 1.2984... Val Loss: 1.5628\n",
            "Epoch: 43/50... Step: 5320... Loss: 1.2985... Val Loss: 1.5654\n",
            "Epoch: 43/50... Step: 5330... Loss: 1.2770... Val Loss: 1.5574\n",
            "Epoch: 43/50... Step: 5340... Loss: 1.2941... Val Loss: 1.5592\n",
            "Epoch: 43/50... Step: 5350... Loss: 1.3246... Val Loss: 1.5599\n",
            "Epoch: 43/50... Step: 5360... Loss: 1.3147... Val Loss: 1.5549\n",
            "Epoch: 43/50... Step: 5370... Loss: 1.3007... Val Loss: 1.5521\n",
            "Epoch: 43/50... Step: 5380... Loss: 1.2809... Val Loss: 1.5552\n",
            "Epoch: 43/50... Step: 5390... Loss: 1.2911... Val Loss: 1.5538\n",
            "Epoch: 43/50... Step: 5400... Loss: 1.2927... Val Loss: 1.5521\n",
            "Epoch: 43/50... Step: 5410... Loss: 1.3150... Val Loss: 1.5601\n",
            "Epoch: 44/50... Step: 5420... Loss: 1.3198... Val Loss: 1.5582\n",
            "Epoch: 44/50... Step: 5430... Loss: 1.2906... Val Loss: 1.5581\n",
            "Epoch: 44/50... Step: 5440... Loss: 1.2756... Val Loss: 1.5572\n",
            "Epoch: 44/50... Step: 5450... Loss: 1.3068... Val Loss: 1.5567\n",
            "Epoch: 44/50... Step: 5460... Loss: 1.3112... Val Loss: 1.5536\n",
            "Epoch: 44/50... Step: 5470... Loss: 1.3036... Val Loss: 1.5560\n",
            "Epoch: 44/50... Step: 5480... Loss: 1.3131... Val Loss: 1.5546\n",
            "Epoch: 44/50... Step: 5490... Loss: 1.3151... Val Loss: 1.5531\n",
            "Epoch: 44/50... Step: 5500... Loss: 1.2644... Val Loss: 1.5561\n",
            "Epoch: 44/50... Step: 5510... Loss: 1.2985... Val Loss: 1.5538\n",
            "Epoch: 44/50... Step: 5520... Loss: 1.2445... Val Loss: 1.5544\n",
            "Epoch: 44/50... Step: 5530... Loss: 1.3367... Val Loss: 1.5521\n",
            "Epoch: 44/50... Step: 5540... Loss: 1.2789... Val Loss: 1.5587\n",
            "Epoch: 45/50... Step: 5550... Loss: 1.2584... Val Loss: 1.5525\n",
            "Epoch: 45/50... Step: 5560... Loss: 1.2708... Val Loss: 1.5554\n",
            "Epoch: 45/50... Step: 5570... Loss: 1.2918... Val Loss: 1.5567\n",
            "Epoch: 45/50... Step: 5580... Loss: 1.2813... Val Loss: 1.5554\n",
            "Epoch: 45/50... Step: 5590... Loss: 1.3294... Val Loss: 1.5524\n",
            "Epoch: 45/50... Step: 5600... Loss: 1.2512... Val Loss: 1.5588\n",
            "Epoch: 45/50... Step: 5610... Loss: 1.3100... Val Loss: 1.5575\n",
            "Epoch: 45/50... Step: 5620... Loss: 1.2829... Val Loss: 1.5570\n",
            "Epoch: 45/50... Step: 5630... Loss: 1.2850... Val Loss: 1.5586\n",
            "Epoch: 45/50... Step: 5640... Loss: 1.2792... Val Loss: 1.5553\n",
            "Epoch: 45/50... Step: 5650... Loss: 1.2833... Val Loss: 1.5551\n",
            "Epoch: 45/50... Step: 5660... Loss: 1.2785... Val Loss: 1.5566\n",
            "Epoch: 45/50... Step: 5670... Loss: 1.3447... Val Loss: 1.5630\n",
            "Epoch: 46/50... Step: 5680... Loss: 1.2984... Val Loss: 1.5576\n",
            "Epoch: 46/50... Step: 5690... Loss: 1.2913... Val Loss: 1.5551\n",
            "Epoch: 46/50... Step: 5700... Loss: 1.2957... Val Loss: 1.5582\n",
            "Epoch: 46/50... Step: 5710... Loss: 1.3059... Val Loss: 1.5523\n",
            "Epoch: 46/50... Step: 5720... Loss: 1.2806... Val Loss: 1.5543\n",
            "Epoch: 46/50... Step: 5730... Loss: 1.2730... Val Loss: 1.5523\n",
            "Epoch: 46/50... Step: 5740... Loss: 1.3140... Val Loss: 1.5510\n",
            "Epoch: 46/50... Step: 5750... Loss: 1.2964... Val Loss: 1.5509\n",
            "Epoch: 46/50... Step: 5760... Loss: 1.2831... Val Loss: 1.5492\n",
            "Epoch: 46/50... Step: 5770... Loss: 1.2806... Val Loss: 1.5491\n",
            "Epoch: 46/50... Step: 5780... Loss: 1.2664... Val Loss: 1.5490\n",
            "Epoch: 46/50... Step: 5790... Loss: 1.2543... Val Loss: 1.5564\n",
            "Epoch: 47/50... Step: 5800... Loss: 1.3129... Val Loss: 1.5505\n",
            "Epoch: 47/50... Step: 5810... Loss: 1.2812... Val Loss: 1.5509\n",
            "Epoch: 47/50... Step: 5820... Loss: 1.2865... Val Loss: 1.5514\n",
            "Epoch: 47/50... Step: 5830... Loss: 1.2877... Val Loss: 1.5544\n",
            "Epoch: 47/50... Step: 5840... Loss: 1.2950... Val Loss: 1.5492\n",
            "Epoch: 47/50... Step: 5850... Loss: 1.2854... Val Loss: 1.5501\n",
            "Epoch: 47/50... Step: 5860... Loss: 1.2995... Val Loss: 1.5525\n",
            "Epoch: 47/50... Step: 5870... Loss: 1.2811... Val Loss: 1.5496\n",
            "Epoch: 47/50... Step: 5880... Loss: 1.2421... Val Loss: 1.5568\n",
            "Epoch: 47/50... Step: 5890... Loss: 1.2476... Val Loss: 1.5525\n",
            "Epoch: 47/50... Step: 5900... Loss: 1.2755... Val Loss: 1.5538\n",
            "Epoch: 47/50... Step: 5910... Loss: 1.2354... Val Loss: 1.5515\n",
            "Epoch: 47/50... Step: 5920... Loss: 1.2632... Val Loss: 1.5596\n",
            "Epoch: 48/50... Step: 5930... Loss: 1.2912... Val Loss: 1.5519\n",
            "Epoch: 48/50... Step: 5940... Loss: 1.2788... Val Loss: 1.5611\n",
            "Epoch: 48/50... Step: 5950... Loss: 1.2678... Val Loss: 1.5560\n",
            "Epoch: 48/50... Step: 5960... Loss: 1.2509... Val Loss: 1.5514\n",
            "Epoch: 48/50... Step: 5970... Loss: 1.2758... Val Loss: 1.5510\n",
            "Epoch: 48/50... Step: 5980... Loss: 1.2889... Val Loss: 1.5528\n",
            "Epoch: 48/50... Step: 5990... Loss: 1.2936... Val Loss: 1.5516\n",
            "Epoch: 48/50... Step: 6000... Loss: 1.2777... Val Loss: 1.5494\n",
            "Epoch: 48/50... Step: 6010... Loss: 1.2657... Val Loss: 1.5524\n",
            "Epoch: 48/50... Step: 6020... Loss: 1.2689... Val Loss: 1.5525\n",
            "Epoch: 48/50... Step: 6030... Loss: 1.2743... Val Loss: 1.5469\n",
            "Epoch: 48/50... Step: 6040... Loss: 1.2927... Val Loss: 1.5554\n",
            "Epoch: 49/50... Step: 6050... Loss: 1.2876... Val Loss: 1.5565\n",
            "Epoch: 49/50... Step: 6060... Loss: 1.2598... Val Loss: 1.5532\n",
            "Epoch: 49/50... Step: 6070... Loss: 1.2602... Val Loss: 1.5507\n",
            "Epoch: 49/50... Step: 6080... Loss: 1.2762... Val Loss: 1.5503\n",
            "Epoch: 49/50... Step: 6090... Loss: 1.2877... Val Loss: 1.5491\n",
            "Epoch: 49/50... Step: 6100... Loss: 1.2855... Val Loss: 1.5498\n",
            "Epoch: 49/50... Step: 6110... Loss: 1.2938... Val Loss: 1.5498\n",
            "Epoch: 49/50... Step: 6120... Loss: 1.2934... Val Loss: 1.5442\n",
            "Epoch: 49/50... Step: 6130... Loss: 1.2390... Val Loss: 1.5504\n",
            "Epoch: 49/50... Step: 6140... Loss: 1.2713... Val Loss: 1.5484\n",
            "Epoch: 49/50... Step: 6150... Loss: 1.2289... Val Loss: 1.5487\n",
            "Epoch: 49/50... Step: 6160... Loss: 1.3048... Val Loss: 1.5520\n",
            "Epoch: 49/50... Step: 6170... Loss: 1.2496... Val Loss: 1.5583\n",
            "Epoch: 50/50... Step: 6180... Loss: 1.2469... Val Loss: 1.5507\n",
            "Epoch: 50/50... Step: 6190... Loss: 1.2424... Val Loss: 1.5513\n",
            "Epoch: 50/50... Step: 6200... Loss: 1.2648... Val Loss: 1.5533\n",
            "Epoch: 50/50... Step: 6210... Loss: 1.2664... Val Loss: 1.5519\n",
            "Epoch: 50/50... Step: 6220... Loss: 1.3041... Val Loss: 1.5488\n",
            "Epoch: 50/50... Step: 6230... Loss: 1.2247... Val Loss: 1.5530\n",
            "Epoch: 50/50... Step: 6240... Loss: 1.2846... Val Loss: 1.5507\n",
            "Epoch: 50/50... Step: 6250... Loss: 1.2697... Val Loss: 1.5540\n",
            "Epoch: 50/50... Step: 6260... Loss: 1.2694... Val Loss: 1.5546\n",
            "Epoch: 50/50... Step: 6270... Loss: 1.2584... Val Loss: 1.5523\n",
            "Epoch: 50/50... Step: 6280... Loss: 1.2609... Val Loss: 1.5541\n",
            "Epoch: 50/50... Step: 6290... Loss: 1.2521... Val Loss: 1.5522\n",
            "Epoch: 50/50... Step: 6300... Loss: 1.3074... Val Loss: 1.5578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXl-HMaTEFC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = 'rnn_x_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvfewtNwHnyo",
        "colab_type": "text"
      },
      "source": [
        "## **Making Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqytLH8PHrtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFUgxpfsKRTq",
        "colab_type": "text"
      },
      "source": [
        "### Priming and generating text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yXbHc24KJ8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(net, size, prime='Line', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWnkC6boKVTt",
        "colab_type": "code",
        "outputId": "8bacd293-8cbe-476b-db25-16d528ea3b98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "print(sample(net, 1000, prime='Line', top_k=5))"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Line at all the door\n",
            "I'd say it is that you can bral your love \n",
            "You know you know that I've looked to live tight\n",
            "The same, way I can see you with you \n",
            "When I long to spend your love is the reatoors\n",
            "And I can stop a turning far\n",
            "\n",
            "When you know, that you do\n",
            "\n",
            "Well in my breath to be\n",
            "\n",
            "I keep trying it all I can stand to take a man tight\n",
            "There's no man thit is I need\n",
            "\n",
            "I wanna stop the world to you\n",
            "\n",
            "Take this way I'm gonna do.\n",
            "I'll never let you stop\n",
            "\n",
            "There's a leather a little thing\n",
            "\n",
            "You're my life, before the right\n",
            "It came always tonight, they can leave to stand to be\n",
            "Your love is the world)\n",
            "An that's my looking for my best arally\n",
            "I'd get another love so long \n",
            "I've got your love to my life to me \n",
            "I know is wait on you\n",
            "And will the much a life\n",
            "\n",
            "Well all the loving though \n",
            "And you're love away\n",
            "\n",
            "I wanna get it in love, I know\n",
            "I wanna leave me\n",
            "\n",
            "All I love you\n",
            "\n",
            "There's no minigh of you shaking around through\n",
            "\n",
            "Your soul, then I'm a moneite is to me\n",
            "I lost to be the mirds where I will be what wonder\n",
            "\n",
            "I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiC9D68eKmzi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c9e9580-bf82-43ce-bd74-bd1accd42faa"
      },
      "source": [
        "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
        "with open('rnn_x_epoch.net', 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "    \n",
        "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2y89TBIKnQQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "219eba7a-47eb-45b4-e77d-d2e69064d068"
      },
      "source": [
        "# Sample using a loaded model\n",
        "print(sample(loaded, 1000,  prime=\"The sky above\", top_k=5))"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sky above the way I'll be all of the mind\n",
            "\n",
            "And this munting me for you\n",
            "I'll be a lot of someone\n",
            "The our hounds together\n",
            "\n",
            "And I'm gonna be mine in, yeah, yeah\n",
            "\n",
            "I don't stop a belien\n",
            "I don't wanna know it's all I\n",
            "There are the one I would be all a love so life\n",
            "\n",
            "There's not nothing you're the one that what I can't lose too\n",
            "I wanna be all this searsh that I won’t love to give\n",
            "Your heart to be with me\n",
            "I wanna give you all that I live\n",
            "I'll be the world was together and make you feel\n",
            "I wanna go to live is all, the world of me\n",
            "\n",
            "And the man in my life\n",
            "\n",
            "I know, I lay make your heart\n",
            "It's the want that that wants\n",
            "A showed a moments that you're goodbyen you comes baby\n",
            "\n",
            "I don't wanna be your eyes\n",
            "I want you to deart of love\n",
            "Ain't nothing they want, it’s sometimes you weal the way I could)\n",
            "You got my hand on\n",
            "\n",
            "When you're love is feels the what\n",
            "And you can be a lifet you\n",
            "\n",
            "This thiss it all I want to live\n",
            "The way you will never be my body\n",
            "\n",
            "I'll bight inside to be and ment of love\n",
            "I don't know you go that\n",
            "\n",
            "Watch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72asTe0vhLlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}